{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef368f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from knowledge_propagation.utils import io\n",
    "\n",
    "from inference.devapi import gptqa\n",
    "from utils.io_utils import jload, jdump\n",
    "from tasks.quality import QuALITY\n",
    "from utils.io_utils import set_openai_key\n",
    "import random\n",
    "\n",
    "from utils.prompt_utils import (\n",
    "    format_name, uncapitalize_first, second_last_character,\n",
    "    OPENAI_API_SYSTEM_QUALITY_GENERATE_ENTITIES,\n",
    "    OPENAI_API_SYSTEM_QUALITY_GENERATE_TWO_ENTITY_RELATIONS,\n",
    "    OPENAI_API_SYSTEM_QUALITY_GENERATE_THREE_ENTITY_RELATIONS,\n",
    "    QUALITY_FEW_SHOT_COT_PROMPT\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd01bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entities(document_content: str,\n",
    "                      system_message: str,\n",
    "                      openai_model: str):\n",
    "    prompt = f\"\"\"\n",
    "    ### Document Content:\n",
    "    {document_content}\n",
    "    \"\"\"\n",
    "    can_read_entities = None\n",
    "    while not can_read_entities:\n",
    "        try:\n",
    "            completion = gptqa(prompt,\n",
    "                               openai_model,\n",
    "                               system_message,\n",
    "                               json_format=True)\n",
    "            response = json.loads(completion)\n",
    "            can_read_entities = response['entities']\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate entities: {str(e)}\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2663378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4-turbo\"\n",
    "random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f14572",
   "metadata": {},
   "source": [
    "### Randomly create test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test_data = io.load_jsonlines(\"/home/zliu/zliu/KE-by-CP/data/debug_meta_train/syn_data_neurips/4Ktrain_data_100percent_frozen/test_text_data_id_entity152_rel31.jsonl\")\n",
    "np.random.shuffle(id_test_data)\n",
    "# io.dump_jsonlines(id_test_data[:50], \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82162c",
   "metadata": {},
   "source": [
    "### Extract entities from each test doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c88ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test_samples = io.load_jsonlines(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b67122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29d2f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:55<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "test_doc_entities = []\n",
    "\n",
    "for id_test_sample in tqdm(id_test_samples):\n",
    "    entities = generate_entities(\n",
    "        id_test_sample[\"text\"],\n",
    "        OPENAI_API_SYSTEM_QUALITY_GENERATE_ENTITIES,\n",
    "        model_name\n",
    "    )\n",
    "    test_doc_entities.append(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b5932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ryan Kelly was born in India. He spent most of his adult life in Oman. After retirement, he lived in Norway and passed away.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7e5fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.dump_jsonlines(test_doc_entities, \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_entities.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce7ebef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_test_samples) == len(test_doc_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51c08bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entigraph_inputs = []\n",
    "text2entity_extract = {}\n",
    "\n",
    "for sample, entities in zip(id_test_samples, test_doc_entities):\n",
    "    entigraph_inputs.append({\n",
    "        \"text\": sample[\"text\"],\n",
    "        **entities\n",
    "    })\n",
    "    text2entity_extract[sample[\"text\"]] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462ba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0d8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.dump_jsonlines(entigraph_inputs, \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_entigraph_inputs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5739a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will act as a knowledge analyzer tasked with dissecting an article provided by the user. Your role involves two main objectives:\n",
      "1. Rephrasing Content: The user will identify two specific entities mentioned in the article. You are required to rephrase the content of the article twice:\n",
      "    * Once, emphasizing the first entity.\n",
      "    * Again, emphasizing the second entity.\n",
      "2. Analyzing Interactions: Discuss how the two specified entities interact within the context of the article.\n",
      "\n",
      "Your responses should provide clear segregation between the rephrased content and the interaction analysis. Ensure each section of the output include sufficient context, ideally referencing the article's title to maintain clarity about the discussion's focus.\n",
      "Here is the format you should follow for your response:\n",
      "\n",
      "### Discussion of <title> in relation to <entity1>\n",
      "<Rephrased content focusing on the first entity>\n",
      "\n",
      "### Discussion of <title> in relation to <entity2>\n",
      "<Rephrased content focusing on the second entity>\n",
      "\n",
      "### Discussion of Interaction between <entity1> and <entity2> in context of <title>\n",
      "<Discussion on how the two entities interact within the article>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_SYSTEM_QUALITY_GENERATE_TWO_ENTITY_RELATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aec885",
   "metadata": {},
   "source": [
    "### Generate prompt for entity pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea394c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5d59509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 481.04it/s]\n"
     ]
    }
   ],
   "source": [
    "entigraph_pair_prompts = []\n",
    "\n",
    "for entigraph_input in tqdm(entigraph_inputs):\n",
    "    pair_list = []\n",
    "    # iterate over pairs of entities and generate relations\n",
    "    document_content = entigraph_input[\"text\"]\n",
    "    entities = entigraph_input[\"entities\"]\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i+1, len(entities)):\n",
    "            pair = (entities[i], entities[j])\n",
    "            pair_list.append(pair)\n",
    "            \n",
    "    for entity1, entity2 in pair_list:\n",
    "        entigraph_pair_prompts.append(\n",
    "            {\n",
    "            \"text\": document_content, \n",
    "            \"summary\": entigraph_input[\"summary\"],\n",
    "            \"aug_type\": \"entity-pair\",\n",
    "            \"entities\": [entity1, entity2],\n",
    "            \"prompt\": f\"{OPENAI_API_SYSTEM_QUALITY_GENERATE_TWO_ENTITY_RELATIONS}\" + \"\\n\\n\" + \\\n",
    "f\"\"\"\n",
    "### Document Content:\n",
    "{document_content}\n",
    "### Entities:\n",
    "- {entity1}\n",
    "- {entity2}\n",
    "\"\"\"\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23de4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 6484.50it/s]\n"
     ]
    }
   ],
   "source": [
    "entigraph_triplet_prompts = []\n",
    "\n",
    "for entigraph_input in tqdm(entigraph_inputs):\n",
    "    # iterate over pairs of entities and generate relations\n",
    "    document_content = entigraph_input[\"text\"]\n",
    "    entities = entigraph_input[\"entities\"]\n",
    "    triple_list = []\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i+1, len(entities)):\n",
    "            for k in range(j+1, len(entities)):\n",
    "                triple = (entities[i], entities[j], entities[k])\n",
    "                triple_list.append(triple)\n",
    "    random.shuffle(triple_list)\n",
    "    for entity1, entity2, entity3 in triple_list:\n",
    "        entigraph_triplet_prompts.append({\n",
    "            \"text\": document_content, \n",
    "            \"summary\": entigraph_input[\"summary\"],\n",
    "            \"aug_type\": \"entity-triplet\",\n",
    "            \"entities\": [entity1, entity2, entity3],\n",
    "            \"prompt\": f\"{OPENAI_API_SYSTEM_QUALITY_GENERATE_THREE_ENTITY_RELATIONS}\" + \"\\n\\n\" + \\\n",
    "f\"\"\"\n",
    "### Document Content:\n",
    "{document_content}\n",
    "### Entities:\n",
    "- {entity1}\n",
    "- {entity2}\n",
    "- {entity3}\n",
    "\"\"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "82ad407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entigraph_prompts = entigraph_pair_prompts + entigraph_triplet_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b71b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a626757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>aug_type</th>\n",
       "      <th>entities</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mia Lewis first wrote about William Wordsworth...</td>\n",
       "      <td>The article traces the academic and profession...</td>\n",
       "      <td>entity-pair</td>\n",
       "      <td>[Mia Lewis, William Wordsworth]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mia Lewis first wrote about William Wordsworth...</td>\n",
       "      <td>The article traces the academic and profession...</td>\n",
       "      <td>entity-pair</td>\n",
       "      <td>[Mia Lewis, Marie Antoinette]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mia Lewis first wrote about William Wordsworth...</td>\n",
       "      <td>The article traces the academic and profession...</td>\n",
       "      <td>entity-pair</td>\n",
       "      <td>[Mia Lewis, Franklin D. Roosevelt]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mia Lewis first wrote about William Wordsworth...</td>\n",
       "      <td>The article traces the academic and profession...</td>\n",
       "      <td>entity-pair</td>\n",
       "      <td>[Mia Lewis, 8th-grade book report]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mia Lewis first wrote about William Wordsworth...</td>\n",
       "      <td>The article traces the academic and profession...</td>\n",
       "      <td>entity-pair</td>\n",
       "      <td>[Mia Lewis, college thesis]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Alexander Mendoza was born in Oman. She spent ...</td>\n",
       "      <td>The article briefly outlines the life of Alexa...</td>\n",
       "      <td>entity-triplet</td>\n",
       "      <td>[Alexander Mendoza, Oman, Thailand]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>Ryan Kelly was born in India. He spent most of...</td>\n",
       "      <td>The article provides a brief overview of Ryan ...</td>\n",
       "      <td>entity-triplet</td>\n",
       "      <td>[Ryan Kelly, India, Oman]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>Ryan Kelly was born in India. He spent most of...</td>\n",
       "      <td>The article provides a brief overview of Ryan ...</td>\n",
       "      <td>entity-triplet</td>\n",
       "      <td>[India, Oman, Norway]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>Ryan Kelly was born in India. He spent most of...</td>\n",
       "      <td>The article provides a brief overview of Ryan ...</td>\n",
       "      <td>entity-triplet</td>\n",
       "      <td>[Ryan Kelly, India, Norway]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>Ryan Kelly was born in India. He spent most of...</td>\n",
       "      <td>The article provides a brief overview of Ryan ...</td>\n",
       "      <td>entity-triplet</td>\n",
       "      <td>[Ryan Kelly, Oman, Norway]</td>\n",
       "      <td>\\nYou will act as a knowledge analyzer tasked ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1946 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Mia Lewis first wrote about William Wordsworth...   \n",
       "1     Mia Lewis first wrote about William Wordsworth...   \n",
       "2     Mia Lewis first wrote about William Wordsworth...   \n",
       "3     Mia Lewis first wrote about William Wordsworth...   \n",
       "4     Mia Lewis first wrote about William Wordsworth...   \n",
       "...                                                 ...   \n",
       "1941  Alexander Mendoza was born in Oman. She spent ...   \n",
       "1942  Ryan Kelly was born in India. He spent most of...   \n",
       "1943  Ryan Kelly was born in India. He spent most of...   \n",
       "1944  Ryan Kelly was born in India. He spent most of...   \n",
       "1945  Ryan Kelly was born in India. He spent most of...   \n",
       "\n",
       "                                                summary        aug_type  \\\n",
       "0     The article traces the academic and profession...     entity-pair   \n",
       "1     The article traces the academic and profession...     entity-pair   \n",
       "2     The article traces the academic and profession...     entity-pair   \n",
       "3     The article traces the academic and profession...     entity-pair   \n",
       "4     The article traces the academic and profession...     entity-pair   \n",
       "...                                                 ...             ...   \n",
       "1941  The article briefly outlines the life of Alexa...  entity-triplet   \n",
       "1942  The article provides a brief overview of Ryan ...  entity-triplet   \n",
       "1943  The article provides a brief overview of Ryan ...  entity-triplet   \n",
       "1944  The article provides a brief overview of Ryan ...  entity-triplet   \n",
       "1945  The article provides a brief overview of Ryan ...  entity-triplet   \n",
       "\n",
       "                                 entities  \\\n",
       "0         [Mia Lewis, William Wordsworth]   \n",
       "1           [Mia Lewis, Marie Antoinette]   \n",
       "2      [Mia Lewis, Franklin D. Roosevelt]   \n",
       "3      [Mia Lewis, 8th-grade book report]   \n",
       "4             [Mia Lewis, college thesis]   \n",
       "...                                   ...   \n",
       "1941  [Alexander Mendoza, Oman, Thailand]   \n",
       "1942            [Ryan Kelly, India, Oman]   \n",
       "1943                [India, Oman, Norway]   \n",
       "1944          [Ryan Kelly, India, Norway]   \n",
       "1945           [Ryan Kelly, Oman, Norway]   \n",
       "\n",
       "                                                 prompt  \n",
       "0     \\nYou will act as a knowledge analyzer tasked ...  \n",
       "1     \\nYou will act as a knowledge analyzer tasked ...  \n",
       "2     \\nYou will act as a knowledge analyzer tasked ...  \n",
       "3     \\nYou will act as a knowledge analyzer tasked ...  \n",
       "4     \\nYou will act as a knowledge analyzer tasked ...  \n",
       "...                                                 ...  \n",
       "1941  \\nYou will act as a knowledge analyzer tasked ...  \n",
       "1942  \\nYou will act as a knowledge analyzer tasked ...  \n",
       "1943  \\nYou will act as a knowledge analyzer tasked ...  \n",
       "1944  \\nYou will act as a knowledge analyzer tasked ...  \n",
       "1945  \\nYou will act as a knowledge analyzer tasked ...  \n",
       "\n",
       "[1946 rows x 5 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(entigraph_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a877477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(entigraph_prompts).to_excel(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_curator_prompt.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "172e7e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Discussion of Mia Lewis's Academic and Professional Journey in relation to Mia Lewis\n",
      "From her early education years, Mia Lewis showcased her budding interest in historical and literary figures. A memorable instance from Mia Lewis's academic journey occurred during her 8th grade when she penned a book report on William Wordsworth, a renowned figure in English literature. This was perhaps one of her initial steps which later guided her towards a deeper exploration of historical personalities and cultural impacts. Her academic dedication extended into her college years and shaped her professional choices, eventually leading her to a career curating museum exhibitions that pay homage to influential historical figures such as Franklin D. Roosevelt.\n",
      "\n",
      "### Discussion of Mia Lewis's Academic and Professional Journey in relation to William Wordsworth\n",
      "Among the notable historical figures Mia Lewis has explored, William Wordsworth stands out as a pivotal subject of interest. Her engagement with Wordsworth began early, specifically marked by a book report she wrote in the 8th grade, which was not merely an assignment but a significant early exposition into the poetic brilliance of Wordsworth. This early academic encounter with Wordsworth's work established a foundational interest that spanned her scholarly activities, subsequently influencing her cultural and historical endeavors.\n",
      "\n",
      "### Discussion of Interaction between Mia Lewis and William Wordsworth in context of Mia Lewis's Academic and Professional Journey\n",
      "The interaction between Mia Lewis and William Wordsworth in the context of her academic and professional journey illuminates a continuous thread of influence and inspiration drawn from historical and literary figures. Initially, Wordsworth's work acts as an academic catalyst in Mia Lewis's life, sparking a profound interest in literary and historical studies through her 8th-grade book report. This engagement not only signifies an academic exercise but also a deeper, lingering influence that permeates her academic pursuits and shapes her career path. Wordsworth, as a subject of her early scholarly attention, represents the beginnings of a thematic exploration that extends into her professional life, where she honors influential figures through museum exhibitions. Lewis’s trajectory from reading and writing about a literary figure like Wordsworth to curating historical exhibitions showcases a dynamic interaction where historical appreciation impacts professional and intellectual growth. This synergy underscores the profound role literary analysis and historical appreciation play in molding scholarly and career paths.\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_excel(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_curator_prompt_sample_generated.xlsx\").iloc[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b96e2fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1946"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entigraph_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce558a9",
   "metadata": {},
   "source": [
    "### Format the augmentation to be compatible with codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d85be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.remove_last_extension(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_curator_prompt_sample.xlsx\")\n",
    "df = pd.read_excel(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample_curator_prompt_generated.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "entigraph_raw_dir = \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample/entigraph\"\n",
    "os.makedirs(entigraph_raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83d3dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "cat_order = [\"entity-pair\", \"entity-triplet\",]\n",
    "aug_type = CategoricalDtype(categories=cat_order, ordered=True)\n",
    "\n",
    "for t, sub_df in df.groupby(\"text\"):\n",
    "    entities = text2entity_extract[t]\n",
    "    sub_df[\"aug_type\"] = sub_df[\"aug_type\"].astype(aug_type)\n",
    "    output = [\n",
    "        entities[\"entities\"],\n",
    "        entities[\"summary\"],\n",
    "        *sub_df[\"completion\"].to_list()\n",
    "    ]\n",
    "    io.dump_json(output, f\"{entigraph_raw_dir}/{i}.json\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61d8ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_raw_dir = \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample/naive\"\n",
    "os.makedirs(naive_raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "551f6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for t, sub_df in df.groupby(\"text\"):\n",
    "    entities = text2entity_extract[t]\n",
    "    output = [\n",
    "        entities[\"entities\"],\n",
    "        t\n",
    "    ]\n",
    "    io.dump_json(output, f\"{naive_raw_dir}/{i}.json\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "705be5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Discussion of Amber Media Ltd.'s Language Expansion in relation to Persian (Farsi)\n",
      "Amber Media Ltd. initiated its service offerings in the Persian (Farsi) language. This foundational choice positioned the company to establish a strong regional presence initially, serving a demographic familiar with Persian. Focusing on Farsi was a strategic move to capture a niche market right from the start, setting the stage for its later expansions.\n",
      "\n",
      "### Discussion of Amber Media Ltd.'s Language Expansion in relation to Arabic\n",
      "After establishing a foothold with its Persian (Farsi) language services, Amber Media Ltd. expanded its linguistic range to include Arabic. This addition significantly broadened its market reach, tapping into a larger and diverse speaker base across multiple countries. The introduction of Arabic allowed Amber Media Ltd. to engage with a vast audience, enhancing its impact and presence in the Middle Eastern markets.\n",
      "\n",
      "### Discussion of Amber Media Ltd.'s Language Expansion in relation to Portuguese\n",
      "Amber Media Ltd. further amplified its global footprint by launching a major initiative to support the Portuguese language, marking a pivotal milestone in the company’s expansion strategy. By adopting Portuguese, Amber Media was able to access markets in both Portugal and Brazil, as well as other Portuguese-speaking regions, thus capitalizing on the economic and cultural opportunities in these areas.\n",
      "\n",
      "### Discussion of Interaction between Persian (Farsi), Arabic, and Portuguese in context of Amber Media Ltd.'s Language Expansion\n",
      "In the article detailing Amber Media Ltd.’s language expansion, Persian (Farsi), Arabic, and Portuguese play crucial roles as strategic tools in the company’s growth plan. Initially, Persian (Farsi) served as the entry point, helping Amber Media cement its initial market and gain operational experience. The subsequent introduction of Arabic represented a natural progression towards engaging a broader, culturally rich market, considerably expanding the audience. Finally, the inclusion of Portuguese was a significant leap towards realizing a more global presence, tapping into the economically diverse Portuguese-speaking regions. These three languages facilitated Amber Media Ltd.'s phased but steady growth from a regional service provider to a significant player with a broader international reach.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(io.load_json(\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample/entigraph/4.json\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3885c0",
   "metadata": {},
   "source": [
    "### Create tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acca63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from utils.io_utils import jload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444f0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _glob_all_json(dir_name: str) -> List[str]:\n",
    "    return glob.glob(f'{dir_name}/*.json') + glob.glob(f'{dir_name}/.*.json')\n",
    "\n",
    "def _get_quality_graph(dir_name: str) -> List[str]:\n",
    "    files = _glob_all_json(dir_name)\n",
    "    result = []\n",
    "    for file in files:\n",
    "        content = jload(file)\n",
    "        result.extend(content[1:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c7de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e2e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_model_name: str)-> AutoTokenizer:\n",
    "    # loading tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name, use_fast=True)\n",
    "    tokenizer.model_max_length=2**20 # this is to hide the token_len>128K wraning\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_list(text_list: List[str], tokenizer_name=\"meta-llama/Meta-Llama-3-8B\") -> List[int]:\n",
    "    \"\"\"\n",
    "    Tokenize the text and return the tokenized text\n",
    "    \"\"\"\n",
    "    random.shuffle(text_list)\n",
    "    tokenizer = get_tokenizer(tokenizer_name)\n",
    "    all_ids = []\n",
    "    for text in tqdm(text_list):\n",
    "        if text:\n",
    "            ids = tokenizer.encode(text) # add_special_tokens=True to add BOS token\n",
    "            ids.append(tokenizer.eos_token_id) # add the end of text token\n",
    "            all_ids.extend(ids)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47c4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_memmap_single(ids: List[int], filename: str, dir_path=\"data/dataset/bins\"):\n",
    "    filename = f'{dir_path}/{filename}'\n",
    "    print(f'Writing to {filename} with length {len(ids)}')\n",
    "    dtype = np.int32\n",
    "    ids_arr = np.array(ids, dtype=dtype)\n",
    "    arr_len = len(ids_arr)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    arr[:] = ids_arr\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_source = \"entigraph\"\n",
    "# text_source = \"naive\"\n",
    "\n",
    "corpus_lst = _get_quality_graph(f\"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/raw/4K_controlled_RE/test_id_sample/{text_source}\")\n",
    "\n",
    "# tokenizer_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# tokenizer_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bea3e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1996/1996 [00:01<00:00, 1045.17it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = tokenize_list(corpus_lst, tokenizer_name=tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fd9ba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "878400"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cae6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/bins/4K_controlled_RE-test_id_sample-entigraph-Qwen2.5-1.5B-Instruct.bin with length 878400\n"
     ]
    }
   ],
   "source": [
    "write_to_memmap_single(tokenized_corpus, filename=f\"4K_controlled_RE-test_id_sample-{text_source}-{os.path.basename(tokenizer_name)}.bin\", dir_path = \"/home/zliu/zliu/Synthetic_Continued_Pretraining_leo/data/dataset/bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054371b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
